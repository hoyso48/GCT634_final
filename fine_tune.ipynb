{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ['HF_HOME'] = '/workspace/GCT634_final/huggingface'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastLanguageModel.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "Unsloth: You selected full finetuning support, but 4bit / 8bit is enabled - disabling LoRA / QLoRA.\n",
      "==((====))==  Unsloth 2025.6.1: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.096 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 9.0. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.30. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using bfloat16 full finetuning which cuts memory usage by 50%.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "770e12dfbcd24aab95e7b92fb1e97108",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/32.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3c8919b766e4f61970fdfd626a47383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baf178ced3f543a991f0a438b9691eb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942152f0e73446d7aa45d35b25be209a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e8aa98a3c1346b5860a80ecfed4c438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.58G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae5d9c1dfca04e2299283d9aa4ea4c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac586665bc4c4a34bffa11172411c341",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/237 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b82119b836af44caa6c99a4a4e8612e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/10.5k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f6b1e08d044bf69302af1394ae7b1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e374f40ce3348f08b66a4e3d9f1742e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e821438db3b54a088887a11869ea23d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed23aacfdcae42cc98e4ea2373c69d0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc2b6556ad80499bb37e5e7e325c3a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d10e5080f50944c08f0b863d603c0e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja:   0%|          | 0.00/4.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "#     \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "#     # 4bit dynamic quants for superior accuracy and low memory use\n",
    "#     \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Phi-4\",\n",
    "#     \"unsloth/Llama-3.1-8B\",\n",
    "#     \"unsloth/Llama-3.2-3B\",\n",
    "#     \"unsloth/Qwen3-4B\",\n",
    "#     \"unsloth/Qwen3-8B\"\n",
    "#     \"unsloth/Qwen3-14B\"\n",
    "#     \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-8B\",\n",
    "    max_seq_length = 768,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,     # 4bit uses much less memory\n",
    "    load_in_8bit = True,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = True, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")\n",
    "# FastLanguageModel.from_pretrained?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "#     lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "#     bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,   # We support rank stabilized LoRA\n",
    "#     loftq_config = None,  # And LoftQ\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_caption = pd.read_csv(\"data/DX7_YAMAHA_train_captions.csv\", index_col=0)\n",
    "train_caption_add = pd.read_csv(\"data/DX7_AllTheWeb_train_captions.csv\", index_col=0)\n",
    "test_caption = pd.read_csv(\"data/DX7_YAMAHA_test_captions.csv\", index_col=0)\n",
    "train_data = pd.read_csv(\"data/DX7_YAMAHA_train.csv\", index_col=0)\n",
    "train_data_add = pd.read_csv(\"data/DX7_AllTheWeb_train.csv\", index_col=0)\n",
    "test_data = pd.read_csv(\"data/DX7_YAMAHA_test.csv\", index_col=0)\n",
    "train_df = pd.merge(train_data, train_caption[['id', 'caption']], on='id', how='left')\n",
    "train_df_add = pd.merge(train_data_add, train_caption_add[['id', 'caption']], on='id', how='left')\n",
    "train_df = pd.concat([train_df, train_df_add])\n",
    "test_df = pd.merge(test_data, test_caption[['id', 'caption']], on='id', how='left')\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[(train_df['inaudible'] == False) & ~train_df['name'].str.contains('NULL') & ~train_df['has_fixed_freqs']]\n",
    "test_df = test_df[(test_df['inaudible'] == False) & ~test_df['name'].str.contains('NULL') & ~test_df['has_fixed_freqs']]\n",
    "print(len(train_df), len(test_df))\n",
    "KEYS_TO_REMOVE = ['name', 'has_fixed_freqs', 'fixed_freq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df[['id', 'caption', 'patch_data']])\n",
    "test_dataset = Dataset.from_pandas(test_df[['id', 'caption', 'patch_data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import zeroshot_prompt\n",
    "import ast\n",
    "from utils import serialize_specs\n",
    "def proprocess(examples, prompt = zeroshot_prompt):\n",
    "    conversations = []\n",
    "    for caption, patch_data in zip(examples[\"caption\"], examples[\"patch_data\"]):\n",
    "        question = prompt.format(prompt=caption)\n",
    "        specs = ast.literal_eval(patch_data)\n",
    "        for key in KEYS_TO_REMOVE:\n",
    "            specs.pop(key, None)\n",
    "        solution = '```python\\n' + 'specs = ' +serialize_specs(specs) + '```\\n'\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : question},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tokenizer.apply_chat_template(\n",
    "    train_dataset.map(proprocess, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "test_dataset = tokenizer.apply_chat_template(\n",
    "    test_dataset.map(proprocess, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "train_dataset = pd.Series(train_dataset)\n",
    "test_dataset = pd.Series(test_dataset)\n",
    "train_dataset.name = \"text\"\n",
    "test_dataset.name = \"text\"\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_dataset)).shuffle()\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lengths = [len(tokenizer(train_dataset[i]['text'])['input_ids']) for i in range(len(train_dataset))]\n",
    "# print(max(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = test_dataset, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps = 100,\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 8,\n",
    "        gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "        warmup_steps = 100,\n",
    "        # warmup_ratio = 0.01,\n",
    "        num_train_epochs = 1, # Set this for 1 full training run.\n",
    "        learning_rate = 1e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        logging_dir = \"logs\",\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "        # bf16 = True,\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.180400\t0.266946\n",
    "#0.203500\t0.259421\n",
    "#  [2270/2288 26:59 < 00:12, 1.40 it/s, Epoch 0.99/1]\n",
    "# Step\tTraining Loss\tValidation Loss\n",
    "# 100\t0.312600\t0.444091\n",
    "# 200\t0.249000\t0.314182\n",
    "# 300\t0.286600\t0.288614\n",
    "# 400\t0.321900\t0.279245\n",
    "# 500\t0.187000\t0.260607\n",
    "# 600\t0.301500\t0.252575\n",
    "# 700\t0.232400\t0.246626\n",
    "# 800\t0.250500\t0.239504\n",
    "# 900\t0.209000\t0.228961\n",
    "# 1000\t0.262700\t0.221790\n",
    "# 1100\t0.295300\t0.215801\n",
    "# 1200\t0.236800\t0.205192\n",
    "# 1300\t0.220500\t0.202858\n",
    "# 1400\t0.211400\t0.195314\n",
    "# 1500\t0.199600\t0.189298\n",
    "# 1600\t0.146000\t0.181624\n",
    "# 1700\t0.230300\t0.176836\n",
    "# 1800\t0.200400\t0.171057\n",
    "# 1900\t0.133000\t0.168132\n",
    "# 2000\t0.173400\t0.165630\n",
    "# 2100\t0.181000\t0.163183\n",
    "# 2200\t0.155600\t0.162581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = test_df.iloc[23]['caption']\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : zeroshot_prompt.format(prompt=caption)}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "print(caption)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    # temperature = 1.0, #, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    # do_sample = False,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydx7 import load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
