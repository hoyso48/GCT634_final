{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hoyeol/miniconda3/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-08 14:14:28.646062: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-06-08 14:14:28.659966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749392068.675592   47115 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749392068.680378   47115 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749392068.693875   47115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749392068.693895   47115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749392068.693897   47115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749392068.693899   47115 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-08 14:14:28.698323: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "# fourbit_models = [\n",
    "#     \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n",
    "#     \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n",
    "\n",
    "#     # 4bit dynamic quants for superior accuracy and low memory use\n",
    "#     \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
    "#     \"unsloth/Phi-4\",\n",
    "#     \"unsloth/Llama-3.1-8B\",\n",
    "#     \"unsloth/Llama-3.2-3B\",\n",
    "#     \"unsloth/Qwen3-4B\",\n",
    "#     \"unsloth/Qwen3-8B\"\n",
    "#     \"unsloth/Qwen3-14B\"\n",
    "#     \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n",
    "# ] # More models at https://huggingface.co/unsloth\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B\",\n",
    "    max_seq_length = 2048,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = False,     # 4bit uses much less memory\n",
    "    load_in_8bit = True,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = True, # We have full finetuning now!\n",
    "    # token = \"hf_...\",      # use one if using gated models\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = FastLanguageModel.get_peft_model(\n",
    "#     model,\n",
    "#     r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "#     target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "#                       \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "#     lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "#     lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "#     bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "#     # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "#     use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "#     random_state = 3407,\n",
    "#     use_rslora = False,   # We support rank stabilized LoRA\n",
    "#     loftq_config = None,  # And LoftQ\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_caption = pd.read_csv(\"/home/hoyeol/projects/GCT634_final/data/DX7_YAMAHA_captions_train.csv\", index_col=0)\n",
    "valid_caption = pd.read_csv(\"/home/hoyeol/projects/GCT634_final/data/DX7_YAMAHA_captions_test.csv\", index_col=0)\n",
    "train_path = pd.read_csv(\"/home/hoyeol/projects/GCT634_final/data/DX7_YAMAHA_train_filtered.csv\", index_col=0)\n",
    "valid_path = pd.read_csv(\"/home/hoyeol/projects/GCT634_final/data/DX7_YAMAHA_test_filtered.csv\", index_col=0)\n",
    "train_df = pd.merge(train_path, train_caption[['id', 'caption']], on='id', how='left')\n",
    "valid_df = pd.merge(valid_path, valid_caption[['id', 'caption']], on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from datasets import Dataset, DatasetDict\n",
    "train_dataset = Dataset.from_pandas(train_df[['id', 'caption', 'patch_data']])\n",
    "valid_dataset = Dataset.from_pandas(valid_df[['id', 'caption', 'patch_data']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prompt import zeroshot_prompt\n",
    "import ast\n",
    "from utils import serialize_specs\n",
    "def proprocess(examples, prompt = zeroshot_prompt):\n",
    "    conversations = []\n",
    "    for caption, patch_data in zip(examples[\"caption\"], examples[\"patch_data\"]):\n",
    "        question = prompt.format(prompt=caption)\n",
    "        specs = ast.literal_eval(patch_data)\n",
    "        specs.pop('name', None)\n",
    "        specs.pop('has_fixed_freqs', None)\n",
    "        solution = '```python\\n' + 'specs = ' +serialize_specs(specs) + '```\\n'\n",
    "        conversations.append([\n",
    "            {\"role\" : \"user\",      \"content\" : question},\n",
    "            {\"role\" : \"assistant\", \"content\" : solution},\n",
    "        ])\n",
    "    return { \"conversations\": conversations, }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:00<00:00, 4792.68 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:00<00:00, 4134.52 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tokenizer.apply_chat_template(\n",
    "    train_dataset.map(proprocess, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "valid_dataset = tokenizer.apply_chat_template(\n",
    "    valid_dataset.map(proprocess, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n",
    "train_dataset = pd.Series(train_dataset)\n",
    "valid_dataset = pd.Series(valid_dataset)\n",
    "train_dataset.name = \"text\"\n",
    "valid_dataset.name = \"text\"\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame(train_dataset)).shuffle()\n",
    "valid_dataset = Dataset.from_pandas(pd.DataFrame(valid_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Find the parameter values for the famous 6-OP(operator) FM synthesizer, the DX7, such that the resulting sound matches the given prompt.\n",
      "### Prompt: Koto, which has a bright, plucked timbre with a resonant decay.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "```python\n",
      "specs = {\n",
      "    'modmatrix': [\n",
      "\t\t[0, 1, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 1, 1, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 1],\n",
      "\t\t[0, 0, 0, 0, 0, 1]\n",
      "    ],\n",
      "    'outmatrix': [1, 0, 1, 0, 0, 0],\n",
      "    'feedback': 7,\n",
      "    'coarse': [3, 4, 1, 1, 4, 1],\n",
      "    'fine': [0, 0, 0, 0, 0, 0],\n",
      "    'detune': [-7, -7, -7, -7, -7, -7],\n",
      "    'transpose': 0,\n",
      "    'ol': [77, 83, 82, 99, 99, 94],\n",
      "    'eg_rate': [\n",
      "\t\t[65, 83, 81, 60, 83, 89],\n",
      "\t\t[70, 37, 28, 64, 68, 62],\n",
      "\t\t[0, 29, 8, 28, 28, 58],\n",
      "\t\t[0, 15, 27, 32, 48, 34]\n",
      "    ],\n",
      "    'eg_level': [\n",
      "\t\t[99, 99, 99, 99, 99, 99],\n",
      "\t\t[88, 90, 76, 92, 83, 92],\n",
      "\t\t[0, 0, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 0]\n",
      "    ],\n",
      "    'sensitivity': [1, 1, 1, 5, 2, 4],\n",
      "}```\n",
      "<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[12]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Tokenizing [\"text\"] (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:06<00:00, 96.59 examples/s] \n",
      "num_proc must be <= 63. Reducing num_proc to 63 for dataset of size 63.\n",
      "Unsloth: Tokenizing [\"text\"] (num_proc=63): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:06<00:00, 10.43 examples/s]\n",
      "Map (num_proc=64): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [00:00<00:00, 1096.41 examples/s]\n",
      "num_proc must be <= 63. Reducing num_proc to 63 for dataset of size 63.\n",
      "Map (num_proc=63): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63/63 [00:00<00:00, 102.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_dataset,\n",
    "    eval_dataset = valid_dataset, # Can set up evaluation!\n",
    "    args = SFTConfig(\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps = 10,\n",
    "        dataset_text_field = \"text\",\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
    "        warmup_steps = 10,\n",
    "        num_train_epochs = 10, # Set this for 1 full training run.\n",
    "        learning_rate = 2e-4, # Reduce to 2e-5 for long training runs\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    ")\n",
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 600 | Num Epochs = 10 | Total steps = 750\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 1,720,574,976/1,720,574,976 (100.00% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='750' max='750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [750/750 14:09, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.655300</td>\n",
       "      <td>0.619199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.372600</td>\n",
       "      <td>0.370808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.350641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.310300</td>\n",
       "      <td>0.343256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.332700</td>\n",
       "      <td>0.338620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.335352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.314800</td>\n",
       "      <td>0.332908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.330187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.342000</td>\n",
       "      <td>0.330670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.327800</td>\n",
       "      <td>0.327052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.324100</td>\n",
       "      <td>0.326589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.353000</td>\n",
       "      <td>0.325332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.327500</td>\n",
       "      <td>0.324016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.285300</td>\n",
       "      <td>0.323097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.322311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.326600</td>\n",
       "      <td>0.321457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.338800</td>\n",
       "      <td>0.322065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.308300</td>\n",
       "      <td>0.321115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.320103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.306300</td>\n",
       "      <td>0.318715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.284100</td>\n",
       "      <td>0.318620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>0.318784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.284700</td>\n",
       "      <td>0.320097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.334600</td>\n",
       "      <td>0.318077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.331100</td>\n",
       "      <td>0.317645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.342800</td>\n",
       "      <td>0.317259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.284000</td>\n",
       "      <td>0.316259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.305300</td>\n",
       "      <td>0.314770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.252800</td>\n",
       "      <td>0.314260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.350800</td>\n",
       "      <td>0.314476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.318100</td>\n",
       "      <td>0.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.298700</td>\n",
       "      <td>0.320630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.298900</td>\n",
       "      <td>0.315847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.311684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.287800</td>\n",
       "      <td>0.312189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.315392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.257300</td>\n",
       "      <td>0.312346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.237700</td>\n",
       "      <td>0.314600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.263700</td>\n",
       "      <td>0.312679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.269800</td>\n",
       "      <td>0.318446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>410</td>\n",
       "      <td>0.266300</td>\n",
       "      <td>0.314516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.273300</td>\n",
       "      <td>0.309975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>0.278900</td>\n",
       "      <td>0.314048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.249200</td>\n",
       "      <td>0.310603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.276000</td>\n",
       "      <td>0.309310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.318293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>470</td>\n",
       "      <td>0.280700</td>\n",
       "      <td>0.313189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.220900</td>\n",
       "      <td>0.312634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>0.268000</td>\n",
       "      <td>0.315353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.241200</td>\n",
       "      <td>0.315202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>0.260800</td>\n",
       "      <td>0.312369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.253500</td>\n",
       "      <td>0.313579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>530</td>\n",
       "      <td>0.203900</td>\n",
       "      <td>0.318908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.252600</td>\n",
       "      <td>0.314135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.238900</td>\n",
       "      <td>0.316676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.236900</td>\n",
       "      <td>0.313918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>0.234500</td>\n",
       "      <td>0.316843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.193200</td>\n",
       "      <td>0.315625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>590</td>\n",
       "      <td>0.240700</td>\n",
       "      <td>0.316130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.320300</td>\n",
       "      <td>0.313690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>610</td>\n",
       "      <td>0.254900</td>\n",
       "      <td>0.320827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.242200</td>\n",
       "      <td>0.316593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.239200</td>\n",
       "      <td>0.317050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.213000</td>\n",
       "      <td>0.318608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.207200</td>\n",
       "      <td>0.316721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.222500</td>\n",
       "      <td>0.317245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>670</td>\n",
       "      <td>0.261100</td>\n",
       "      <td>0.318472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.265500</td>\n",
       "      <td>0.317852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>0.256800</td>\n",
       "      <td>0.318244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.266500</td>\n",
       "      <td>0.319383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>710</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.319564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.315900</td>\n",
       "      <td>0.320167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>730</td>\n",
       "      <td>0.227100</td>\n",
       "      <td>0.319631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.259300</td>\n",
       "      <td>0.318806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.234000</td>\n",
       "      <td>0.318815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Not an error, but Qwen3Model does not accept `num_items_in_batch`.\n",
      "Using gradient accumulation will be very slightly less accurate.\n",
      "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
     ]
    }
   ],
   "source": [
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0.345100\t0.356628"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Electronic pad, which has a lush and evolving timbre with a slow attack and long release, creating a sustained and atmospheric texture.\n",
      "```python\n",
      "specs = {\n",
      "    'modmatrix': [\n",
      "\t\t[0, 1, 0, 0, 0, 0],\n",
      "\t\t[0, 1, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 1, 0, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 0],\n",
      "\t\t[0, 0, 0, 0, 0, 1],\n",
      "    'outmatrix': [1, 0, 0, 0, 1, 0],\n",
      "    'feedback': 5,\n",
      "    'coarse': [1, 1, 1, 1, 1, 1],\n",
      "    'fine': [0, 0, 0, 0, 0, 0],\n",
      "    'detune': [-1, 0, -7, -5, 5, -4],\n",
      "    'transpose': 0,\n",
      "    'ol': [99, 81, 94, 94, 94, 99],\n",
      "    'eg_rate': [\n",
      "\t\t[98, 98, 98, 98, 98, 98],\n",
      "\t\t[95, 76, 76, 98, 98, 82],\n",
      "\t\t[46, 58, 49, 87, 87, 87],\n",
      "\t\t[16, 17, 17, 69, 48, 97]\n",
      "    ],\n",
      "    'eg_level': [\n",
      "\t\t[99, 99, 99, 99, 99, 99],\n",
      "\t\t[95, 99, 99, 99, 99, 98],\n",
      "\t\t[95, 99, 99, 99, 99, 99],\n",
      "\t\t[0, 0, 0, 0, 0, 0]\n",
      "    ],\n",
      "    'sensitivity': [1, 3, 0, 2, 2, 0],\n",
      "}```\n",
      "<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "caption = train_df.iloc[5]['caption']\n",
    "messages = [\n",
    "    {\"role\" : \"user\", \"content\" : zeroshot_prompt.format(prompt=caption)}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize = False,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    "    enable_thinking = False, # Disable thinking\n",
    ")\n",
    "print(caption)\n",
    "\n",
    "from transformers import TextStreamer\n",
    "output = model.generate(\n",
    "    **tokenizer(text, return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 1024, # Increase for longer outputs!\n",
    "    temperature = 1.0, #, top_p = 0.8, top_k = 20, # For non thinking\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to parse specs dictionary: closing parenthesis '}' does not match opening parenthesis '[' on line 2 (<unknown>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mSyntaxError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/GCT634_final/utils.py:171\u001b[39m, in \u001b[36mparse_last_specs\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     result = \u001b[43mast\u001b[49m\u001b[43m.\u001b[49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlast_dict_str\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/ast.py:64\u001b[39m, in \u001b[36mliteral_eval\u001b[39m\u001b[34m(node_or_string)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node_or_string, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m     node_or_string = \u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m \u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43meval\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node_or_string, Expression):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/lib/python3.11/ast.py:50\u001b[39m, in \u001b[36mparse\u001b[39m\u001b[34m(source, filename, mode, type_comments, feature_version)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;66;03m# Else it should be an int giving the minor version for 3.x.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcompile\u001b[39m(source, filename, mode, flags,\n\u001b[32m     51\u001b[39m                _feature_version=feature_version)\n",
      "\u001b[31mSyntaxError\u001b[39m: closing parenthesis '}' does not match opening parenthesis '[' on line 2 (<unknown>, line 28)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse_last_specs, render_from_specs, validate_specs\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m specs = \u001b[43mparse_last_specs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# validate_specs(specs)\u001b[39;00m\n\u001b[32m      5\u001b[39m audio = render_from_specs(specs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/GCT634_final/utils.py:173\u001b[39m, in \u001b[36mparse_last_specs\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m    171\u001b[39m     result = ast.literal_eval(last_dict_str)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFailed to parse specs dictionary: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mValueError\u001b[39m: Failed to parse specs dictionary: closing parenthesis '}' does not match opening parenthesis '[' on line 2 (<unknown>, line 28)"
     ]
    }
   ],
   "source": [
    "from utils import parse_last_specs, render_from_specs, validate_specs\n",
    "import IPython.display as ipd\n",
    "specs = parse_last_specs(tokenizer.decode(output[0], skip_special_tokens=True))\n",
    "# validate_specs(specs)\n",
    "audio = render_from_specs(specs)\n",
    "ipd.Audio(audio, rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
